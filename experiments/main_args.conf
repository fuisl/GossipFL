

# =======================================================================================
# parameters below align with the configs/default.py
# =======================================================================================


# ---------------------------------------------------------------------------- #
# wandb settings
# ---------------------------------------------------------------------------- #
entity="${entity:-hpml-hkbu}"
project="${project:-test}"
wandb_upload_client_list="${wandb_upload_client_list:-[0,1]}" # 0 is the server
wandb_save_record_dataframe="${wandb_save_record_dataframe:-False}"
wandb_offline="${wandb_offline:-False}"
wandb_record="${wandb_record:-True}"


wandb_args=" entity $entity project $project \
wandb_upload_client_list $wandb_upload_client_list \
wandb_save_record_dataframe $wandb_save_record_dataframe \
wandb_offline $wandb_offline wandb_record $wandb_record "
# ---------------------------------------------------------------------------- #
# mode settings
# ---------------------------------------------------------------------------- #
# distributed or standalone
mode="${mode:-distributed}"

# ---------------------------------------------------------------------------- #
# distributed settings
# ---------------------------------------------------------------------------- #
client_num_in_total="${client_num_in_total:-4}"
client_num_per_round="${client_num_per_round:-4}"
instantiate_all="${instantiate_all:-True}"
clear_buffer="${clear_buffer:-True}"


distributed_args=" client_num_in_total $client_num_in_total  client_num_per_round $client_num_per_round \
instantiate_all $instantiate_all clear_buffer $clear_buffer "
# ---------------------------------------------------------------------------- #
# device settings
# ---------------------------------------------------------------------------- #
is_mobile="${is_mobile:-0}"

# ---------------------------------------------------------------------------- #
# cluster settings
# ---------------------------------------------------------------------------- #
rank="${rank:-0}"
client_index="${client_index:-0}"
gpu_server_num="${gpu_server_num:-0}"
gpu_util_file="${gpu_util_file:-None}"
gpu_util_key="${gpu_util_key:-None}"
gpu_util_parse="${gpu_util_parse:-None}"
cluster_name="${cluster_name:-localhost}"

gpu_index="${gpu_index:-0}"   # for centralized training or standalone usage

cluster_args=" rank $rank client_index $client_index  gpu_server_num $gpu_server_num \
gpu_util_file $gpu_util_file gpu_util_key $gpu_util_key  gpu_util_parse $gpu_util_parse \
cluster_name $cluster_name  gpu_index $gpu_index "
# ---------------------------------------------------------------------------- #
# task settings
# ---------------------------------------------------------------------------- #
# ["classification", "stackoverflow_lr", "ptb"]
task="${task:-classification}"



# ---------------------------------------------------------------------------- #
# dataset
# ---------------------------------------------------------------------------- #
dataset="${dataset:-cifar10}"
dataset_aug="${dataset_aug:-default}"
dataset_resize="${dataset_resize:-False}"
dataset_load_image_size="${dataset_load_image_size:-32}"
num_classes="${num_classes:-10}"
data_efficient_load="${data_efficient_load:-True}"
data_save_memory_mode="${data_save_memory_mode:-False}"
data_dir="${data_dir:-./../../../data/cifar10}"
partition_method="${partition_method:-iid}"
partition_alpha="${partition_alpha:-0.5}"
dirichlet_min_p="${dirichlet_min_p:-None}"
dirichlet_balance="${dirichlet_balance:-False}"
load_multiple_generative_dataset_list="${load_multiple_generative_dataset_list:-['style_GAN_init']}"
if_timm_dataset="${if_timm_dataset:-False}"
data_load_num_workers="${data_load_num_workers:-4}"

an4_audio_path="${an4_audio_path:-no}"
lstm_num_steps="${lstm_num_steps:-35}"
lstm_clip_grad="${lstm_clip_grad:-True}"
lstm_clip_grad_thres="${lstm_clip_grad_thres:-0.25}"
lstm_embedding_dim="${lstm_embedding_dim:-8}"
lstm_hidden_size="${lstm_hidden_size:-256}"

dataset_args=" dataset $dataset  dataset_aug $dataset_aug  dataset_resize $dataset_resize \
dataset_load_image_size $dataset_load_image_size num_classes $num_classes \
data_efficient_load $data_efficient_load  data_save_memory_mode $data_save_memory_mode \
data_dir $data_dir  partition_method $partition_method \
partition_alpha $partition_alpha dirichlet_min_p $dirichlet_min_p dirichlet_balance $dirichlet_balance \
load_multiple_generative_dataset_list $load_multiple_generative_dataset_list \
if_timm_dataset $if_timm_dataset  data_load_num_workers $data_load_num_workers \
an4_audio_path $an4_audio_path \
lstm_num_steps $lstm_num_steps lstm_clip_grad $lstm_clip_grad  lstm_clip_grad_thres $lstm_clip_grad_thres \
lstm_embedding_dim  $lstm_embedding_dim   lstm_hidden_size  $lstm_hidden_size "
# ---------------------------------------------------------------------------- #
# data sampler
# ---------------------------------------------------------------------------- #
data_sampler="${data_sampler:-Random}"

imbalance_beta="${imbalance_beta:-0.9999}"
imbalance_beta_min="${imbalance_beta_min:-0.8}"
imbalance_beta_decay_rate="${imbalance_beta_decay_rate:-0.992}"
# ["global_round", "local_round", "epoch"]
imbalance_beta_decay_type="${imbalance_beta_decay_type:-global_round}"
imbalance_sample_warmup_rounds="${imbalance_sample_warmup_rounds:-0}"

data_sampler_args=" data_sampler $data_sampler imbalance_beta $imbalance_beta  \
imbalance_beta_min $imbalance_beta_min imbalance_beta_decay_rate $imbalance_beta_decay_rate \
imbalance_beta_decay_type $imbalance_beta_decay_type \
imbalance_sample_warmup_rounds $imbalance_sample_warmup_rounds "


# ---------------------------------------------------------------------------- #
# data_preprocessing
# ---------------------------------------------------------------------------- #
data_transform="${data_transform:-NormalTransform}"

data_preprocessing_args=" data_transform  $data_transform  "



# ---------------------------------------------------------------------------- #
# checkpoint_save
# ---------------------------------------------------------------------------- #
checkpoint_save="${checkpoint_save:-False}"
checkpoint_save_model="${checkpoint_save_model:-False}"
checkpoint_save_optim="${checkpoint_save_optim:-False}"
checkpoint_save_train_metric="${checkpoint_save_train_metric:-False}"
checkpoint_save_test_metric="${checkpoint_save_test_metric:-False}"
checkpoint_root_path="${checkpoint_root_path:-./checkpoints/}"
checkpoint_epoch_list="${checkpoint_epoch_list:-[10,20,30]}"
checkpoint_file_name_save_list=${checkpoint_file_name_save_list:-"['mode','algorithm','model','dataset','batch_size','lr','sched',\
'partition_method','partition_alpha','pretrained'\
]"}

checkpoint_args=" checkpoint_save $checkpoint_save  checkpoint_save_model $checkpoint_save_model \
checkpoint_save_optim $checkpoint_save_optim  checkpoint_save_train_metric $checkpoint_save_train_metric \
checkpoint_save_test_metric $checkpoint_save_test_metric  checkpoint_root_path $checkpoint_root_path \
checkpoint_epoch_list $checkpoint_epoch_list  checkpoint_file_name_save_list $checkpoint_file_name_save_list"



# ---------------------------------------------------------------------------- #
# record config
# ---------------------------------------------------------------------------- #
record_dataframe="${record_dataframe:-False}"
record_level="${record_level:-epoch}" # iteration

record_args=" record_dataframe $record_dataframe  record_level $record_level"


# ---------------------------------------------------------------------------- #
# Losses track
# ---------------------------------------------------------------------------- #
losses_track="${losses_track:-False}"
losses_track_client_list="${losses_track_client_list:-[0,1]}"
losses_curve_2model="${losses_curve_2model:-False}"
losses_curve_2model_selected_client="${losses_curve_2model_selected_client:-[0,1]}"
losses_curve_2model_comm_round_list="${losses_curve_2model_comm_round_list:-[0,30,50,100,150,199]}"


losses_check_args=" losses_track $losses_track \
losses_track_client_list $losses_track_client_list \
losses_curve_2model $losses_curve_2model \
losses_curve_2model_selected_client $losses_curve_2model_selected_client \
losses_curve_2model_comm_round_list $losses_curve_2model_comm_round_list "

# ---------------------------------------------------------------------------- #
# param_track
# ---------------------------------------------------------------------------- #
param_track="${param_track:-False}"
param_nonzero_ratio="${param_nonzero_ratio:-1.0}"
param_track_with_training="${param_track_with_training:-True}"
param_track_max_iters="${param_track_max_iters:-'max'}"    # means whole dataset

# Use these three args alternatively to decide which layers should be tracked.
param_track_layers_list="${param_track_layers_list:-[]}"
param_track_layers_length="${param_track_layers_length:--1}"
param_track_types="${param_track_types:-['Conv2d','Linear']}"  #  [""] or   ["Conv2d","Linear"]

param_track_wandb_print_layers="${param_track_wandb_print_layers:--1}" # determine how many layers to output, -1 means all
param_track_save_pth_epoch_list="${param_track_save_pth_epoch_list:-[0,1,2,3,4,9,14,19,24,29,39,59,79,99]}"
param_track_batch_size="${param_track_batch_size:-64}"

param_crt_list="${param_crt_list:-['weight','V_GiWi_aprx']}"

param_track_args=" param_track $param_track \
param_nonzero_ratio $param_nonzero_ratio param_track_with_training $param_track_with_training \
param_track_max_iters $param_track_max_iters \
param_track_layers_list $param_track_layers_list param_track_layers_length $param_track_layers_length \
param_track_types $param_track_types \
param_track_wandb_print_layers $param_track_wandb_print_layers param_track_save_pth_epoch_list $param_track_save_pth_epoch_list \
param_track_batch_size $param_track_batch_size \
param_crt_list $param_crt_list"




# ---------------------------------------------------------------------------- #
# model
# ---------------------------------------------------------------------------- #
model="${model:-resnet20}"
model_input_channels="${model_input_channels:-3}"
model_out_feature="${model_out_feature:-False}"
model_out_feature_layer="${model_out_feature_layer:-last}"
model_feature_dim="${model_feature_dim:-512}"
model_output_dim="${model_output_dim:-10}"
pretrained="${pretrained:-False}"
pretrained_dir="${pretrained_dir:-no}"

# refer to https://github.com/kevinhsieh/non_iid_dml/blob/master/apps/caffe/examples/cifar10/5parts/resnetgn20_train_val.prototxt.template
group_norm_num="${group_norm_num:-0}"



model_args=" model  $model   model_input_channels  $model_input_channels \
model_out_feature $model_out_feature  model_out_feature_layer $model_out_feature_layer \
model_feature_dim $model_feature_dim  model_output_dim $model_output_dim \
pretrained $pretrained   pretrained_dir  $pretrained_dir   \
group_norm_num  $group_norm_num "

# ---------------------------------------------------------------------------- #
# generator
# ---------------------------------------------------------------------------- #
image_resolution="${image_resolution:-32}"
style_gan_ckpt="${style_gan_ckpt:-''}"
style_gan_style_dim="${style_gan_style_dim:-64}"
style_gan_n_mlp="${style_gan_n_mlp:-1}"
style_gan_cmul="${style_gan_cmul:-1}"
style_gan_sample_z_mean="${style_gan_sample_z_mean:-0.3}"
style_gan_sample_z_std="${style_gan_sample_z_std:-0.1}"
vae_decoder_z_dim="${vae_decoder_z_dim:-8}"
vae_decoder_ngf="${vae_decoder_ngf:-64}"


generator_args=" image_resolution $image_resolution \
style_gan_ckpt $style_gan_ckpt  style_gan_style_dim $style_gan_style_dim \
style_gan_n_mlp $style_gan_n_mlp  style_gan_cmul $style_gan_cmul \
style_gan_sample_z_mean $style_gan_sample_z_mean  style_gan_sample_z_std $style_gan_sample_z_std \
vae_decoder_z_dim $vae_decoder_z_dim  vae_decoder_ngf $vae_decoder_ngf "




# ---------------------------------------------------------------------------- #
# generative_dataset
# ---------------------------------------------------------------------------- #
generative_dataset_load_in_memory="${generative_dataset_load_in_memory:-False}"
generative_dataset_pin_memory="${generative_dataset_pin_memory:-True}"
generative_dataset_shared_loader="${generative_dataset_shared_loader:-False}"
generative_dataset_root_path="${generative_dataset_root_path:-'./../../../data/generative'}"
generative_dataset_resize="${generative_dataset_resize:-None}"
generative_dataset_grayscale="${generative_dataset_grayscale:-False}"

generative_dataset_args=" generative_dataset_load_in_memory $generative_dataset_load_in_memory \
generative_dataset_pin_memory $generative_dataset_pin_memory generative_dataset_shared_loader $generative_dataset_shared_loader \
generative_dataset_root_path $generative_dataset_root_path  generative_dataset_resize $generative_dataset_resize \
generative_dataset_grayscale $generative_dataset_grayscale  "




# ---------------------------------------------------------------------------- #
# Average weight
# ---------------------------------------------------------------------------- #
# """[even, datanum, inv_datanum, inv_datanum2datanum, even2datanum,
#         ]
# """
# datanum2others is not considerred for now.
fedavg_avg_weight_type="${fedavg_avg_weight_type:-datanum}"

avg_weight_args=" fedavg_avg_weight_type $fedavg_avg_weight_type "


# ---------------------------------------------------------------------------- #
# Dif local steps
# ---------------------------------------------------------------------------- #
fedavg_local_step_type="${fedavg_local_step_type:-whole}"  # whole, fixed, fixed2whole
fedavg_local_step_fixed_type="${fedavg_local_step_fixed_type:-lowest}" # default, lowest, highest, averaged
fedavg_local_step_num="${fedavg_local_step_num:-10}" # used for the fixed local step default 

dif_local_steps_args=" fedavg_local_step_type $fedavg_local_step_type \
fedavg_local_step_fixed_type $fedavg_local_step_fixed_type \
fedavg_local_step_num $fedavg_local_step_num "


# ---------------------------------------------------------------------------- #
# loss function
# ---------------------------------------------------------------------------- #
loss_fn="${loss_fn:-CrossEntropy}"
normal_supcon_loss="${normal_supcon_loss:-False}"
# ['CrossEntropy', 'nll_loss', 'LDAMLoss', 'local_LDAMLoss',
#        'FocalLoss', 'local_FocalLoss']
imbalance_loss_reweight="${imbalance_loss_reweight:-False}"


loss_fn_args=" loss_fn  $loss_fn  normal_supcon_loss $normal_supcon_loss \
imbalance_loss_reweight $imbalance_loss_reweight "


# ---------------------------------------------------------------------------- #
# trainer
#---------------------------------------------------------------------------- #
# ['normal',  'lstm', 'nas']
trainer_type="${trainer_type:-normal}"

trainer_args=" trainer_type  $trainer_type  "
# ---------------------------------------------------------------------------- #
# algorithm settings
# ---------------------------------------------------------------------------- #
algorithm="${algorithm:-PSGD}"
psgd_exchange="${psgd_exchange:-grad}"  # 'grad', 'model'
psgd_grad_sum="${psgd_grad_sum:-False}"

psgd_grad_debug="${psgd_grad_debug:-False}"
if_get_diff="${if_get_diff:-False}"
exchange_model="${exchange_model:-True}"


# Asynchronous PSGD
# _C.apsgd_exchange = 'grad' # 'grad', 'model' # discarded, use psgd_exchange

# Local SGD
local_round_num="${local_round_num:-4}"

# CHOCO SGD
consensus_stepsize="${consensus_stepsize:-0.5}"
# SAPS FL
bandwidth_type="${bandwidth_type:-random}"   # 'random' 'real'
B_thres="${B_thres:-3.0}"
T_thres="${T_thres:-3}"

# torch_ddp
local_rank="${local_rank:-0}"
init_method="${init_method:-tcp://127.0.0.1:23456}"


# hvd settings and maybe used in future
FP16="${FP16:-False}"
logging_gradients="${logging_gradients:-False}"
merge_threshold="${merge_threshold:-0}"
# horovod version

hvd_origin="${hvd_origin:-False}"
nsteps_update="${nsteps_update:-1}"
# Set it to 1 to turn on momentum_correction
hvd_momentum_correction="${hvd_momentum_correction:-0}"
hvd_is_sparse="${hvd_is_sparse:-False}"





# fedprox
fedprox="${fedprox:-False}"
fedprox_mu="${fedprox_mu:-0.1}"


# fedavg
fedavg_label_smooth="${fedavg_label_smooth:-0.0}"

# scaffold
scaffold="${scaffold:-False}"



algorithm_args=" algorithm $algorithm  psgd_exchange $psgd_exchange  psgd_grad_sum  $psgd_grad_sum \
psgd_grad_debug  $psgd_grad_debug   if_get_diff  $if_get_diff   exchange_model  $exchange_model \
local_round_num  $local_round_num   consensus_stepsize  $consensus_stepsize  \
bandwidth_type  $bandwidth_type  B_thres  $B_thres  T_thres  $T_thres  \
local_rank  $local_rank  init_method  $init_method  FP16  $FP16 logging_gradients  $logging_gradients \
merge_threshold  $merge_threshold  hvd_origin  $hvd_origin  nsteps_update  $nsteps_update \
hvd_momentum_correction  $hvd_momentum_correction  hvd_is_sparse  $hvd_is_sparse \
fedprox $fedprox  fedprox_mu $fedprox_mu \
fedavg_label_smooth $fedavg_label_smooth \
scaffold $scaffold "




# ---------------------------------------------------------------------------- #
# feature noniid measure
# ---------------------------------------------------------------------------- #
x_noniid_measure="${x_noniid_measure:-no}"   #  
x_noniid_measure_dlmodel="${x_noniid_measure_dlmodel:-vgg9}"

x_noniid_measure_args=" x_noniid_measure $x_noniid_measure \
x_noniid_measure_dlmodel $x_noniid_measure_dlmodel "


# ---------------------------------------------------------------------------- #
# compression Including:
# 'topk','randomk', 'gtopk', 'randomkec',  'eftopk', 'gtopkef'
# 'quantize', 'qsgd', 'sign'
# ---------------------------------------------------------------------------- #
compression="${compression:-no}"
compress_ratio="${compress_ratio:-1.0}"
quantize_level="${quantize_level:-32}"
is_biased="${is_biased:-0}"

compression_args=" compression $compression  compress_ratio $compress_ratio  \
quantize_level $quantize_level  is_biased $is_biased "
# ---------------------------------------------------------------------------- #
# optimizer settings
# comm_round is only used in FedAvg.
# ---------------------------------------------------------------------------- #
max_epochs="${max_epochs:-90}"
global_epochs_per_round="${global_epochs_per_round:-1}"
comm_round="${comm_round:-10}"
# Please indicate which optimizer is used, if no, set it as 'no'
client_optimizer="${client_optimizer:-sgd}"
server_optimizer="${server_optimizer:-no}"

batch_size="${batch_size:-32}"
lr="${lr:-0.1}"
wd="${wd:-0.0001}"
momentum="${momentum:-0.9}"
nesterov="${nesterov:-False}"
clip_grad="${clip_grad:-False}"

optimizer_args=" max_epochs $max_epochs global_epochs_per_round $global_epochs_per_round \
comm_round $comm_round \
client_optimizer $client_optimizer  server_optimizer $server_optimizer \
batch_size $batch_size  lr $lr  \
wd $wd   momentum  $momentum  nesterov $nesterov \
clip_grad $clip_grad"
# ---------------------------------------------------------------------------- #
# Learning rate schedule parameters
# ---------------------------------------------------------------------------- #
# no (no scheudler), StepLR MultiStepLR  CosineAnnealingLR
sched="${sched:-no}"
lr_decay_rate="${lr_decay_rate:-0.992}"
step_size="${step_size:-1}"

lr_milestones="${lr_milestones:-[30,60]}"
lr_T_max="${lr_T_max:-10}"
lr_eta_min="${lr_eta_min:-0}"
lr_warmup_type="${lr_warmup_type:-constant}"  # constant or gradual
warmup_epochs="${warmup_epochs:-0}"
lr_warmup_value="${lr_warmup_value:-0.1}"

sched_args=" sched $sched  lr_decay_rate $lr_decay_rate  step_size $step_size \
lr_milestones $lr_milestones  lr_T_max $lr_T_max  lr_eta_min $lr_eta_min  \
lr_warmup_type $lr_warmup_type warmup_epochs $warmup_epochs lr_warmup_value $lr_warmup_value "
# ---------------------------------------------------------------------------- #
# Regularation
# ---------------------------------------------------------------------------- #



# ---------------------------------------------------------------------------- #
# Evaluate settings
# ---------------------------------------------------------------------------- #
frequency_of_the_test="${frequency_of_the_test:-1}"


evaluate_args="  frequency_of_the_test $frequency_of_the_test "
# ---------------------------------------------------------------------------- #
# Robust test
# ---------------------------------------------------------------------------- #
Failure_chance="${Failure_chance:-None}"


# ---------------------------------------------------------------------------- #
# logging
# ---------------------------------------------------------------------------- #
# 'INFO' or 'DEBUG'
level="${level:-INFO}"



# ---------------------------------------------------------------------------- #
# other settings
# ---------------------------------------------------------------------------- #
ci="${ci:-0}"
seed="${seed:-0}"




# =======================================================================================
# get final args
# =======================================================================================




main_args=" $wandb_args \
mode $mode \
$distributed_args \
is_mobile $is_mobile \
$cluster_args \
task $task \
$dataset_args \
$data_sampler_args \
$data_preprocessing_args \
$checkpoint_args \
$record_args \
$losses_check_args \
$param_track_args \
$model_args \
$generator_args \
$generative_dataset_args \
$avg_weight_args \
$dif_local_steps_args \
$loss_fn_args \
$trainer_args \
$algorithm_args \
$x_noniid_measure_args \
$compression_args \
$optimizer_args \
$sched_args \
$evaluate_args \
Failure_chance $Failure_chance \
level $level \
ci $ci \
seed $seed \
"





















