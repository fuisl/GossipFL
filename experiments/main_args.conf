

# =======================================================================================
# parameters below align with the configs/default.py
# =======================================================================================


# ---------------------------------------------------------------------------- #
# wandb settings
# ---------------------------------------------------------------------------- #
entity="${entity:-hpml-hkbu}"
project="${project:-gossipfl}"

wandb_args=" entity $entity project $project "
# ---------------------------------------------------------------------------- #
# mode settings
# ---------------------------------------------------------------------------- #
# distributed or standalone
mode="${mode:-distributed}"

# ---------------------------------------------------------------------------- #
# distributed settings
# ---------------------------------------------------------------------------- #
client_num_in_total="${client_num_in_total:-4}"
client_num_per_round="${client_num_per_round:-4}"


distributed_args=" client_num_in_total $client_num_in_total  client_num_per_round $client_num_per_round "
# ---------------------------------------------------------------------------- #
# device settings
# ---------------------------------------------------------------------------- #
is_mobile="${is_mobile:-0}"

# ---------------------------------------------------------------------------- #
# cluster settings
# ---------------------------------------------------------------------------- #
rank="${rank:-0}"
client_index="${client_index:-0}"
gpu_server_num="${gpu_server_num:-0}"
gpu_util_file="${gpu_util_file:-None}"
gpu_util_key="${gpu_util_key:-None}"
gpu_util_parse="${gpu_util_parse:-None}"
cluster_name="${cluster_name:-localhost}"

gpu_index="${gpu_index:-0}"   # for centralized training or standalone usage

cluster_args=" rank $rank client_index $client_index  gpu_server_num $gpu_server_num \
gpu_util_file $gpu_util_file gpu_util_key $gpu_util_key  gpu_util_parse $gpu_util_parse \
cluster_name $cluster_name  gpu_index $gpu_index "
# ---------------------------------------------------------------------------- #
# task settings
# ---------------------------------------------------------------------------- #
# ["classification", "stackoverflow_lr", "ptb"]
task="${task:-classification}"



# ---------------------------------------------------------------------------- #
# dataset
# ---------------------------------------------------------------------------- #
dataset="${dataset:-cifar10}"
data_dir="${data_dir:-./../../../data/cifar10}"
partition_method="${partition_method:-iid}"
partition_alpha="${partition_alpha:-0.5}"
if_timm_dataset="${if_timm_dataset:-False}"
data_load_num_workers="${data_load_num_workers:-4}"

an4_audio_path="${an4_audio_path:-no}"
lstm_num_steps="${lstm_num_steps:-35}"
lstm_clip_grad="${lstm_clip_grad:-True}"
lstm_clip_grad_thres="${lstm_clip_grad_thres:-0.25}"
lstm_embedding_dim="${lstm_embedding_dim:-8}"
lstm_hidden_size="${lstm_hidden_size:-256}"

dataset_args=" dataset $dataset data_dir $data_dir  partition_method $partition_method \
partition_alpha $partition_alpha if_timm_dataset $if_timm_dataset  data_load_num_workers $data_load_num_workers \
an4_audio_path $an4_audio_path \
lstm_num_steps $lstm_num_steps lstm_clip_grad $lstm_clip_grad  lstm_clip_grad_thres $lstm_clip_grad_thres \
lstm_embedding_dim  $lstm_embedding_dim   lstm_hidden_size  $lstm_hidden_size "
# ---------------------------------------------------------------------------- #
# data sampler
# ---------------------------------------------------------------------------- #
data_sampler="${data_sampler:-Random}"

imbalance_beta="${imbalance_beta:-0.9999}"
imbalance_beta_min="${imbalance_beta_min:-0.8}"
imbalance_beta_decay_rate="${imbalance_beta_decay_rate:-0.992}"
# ["global_round", "local_round", "epoch"]
imbalance_beta_decay_type="${imbalance_beta_decay_type:-global_round}"


data_sampler_args=" data_sampler $data_sampler imbalance_beta $imbalance_beta  \
imbalance_beta_min $imbalance_beta_min imbalance_beta_decay_rate $imbalance_beta_decay_rate \
imbalance_beta_decay_type $imbalance_beta_decay_type  "
# ---------------------------------------------------------------------------- #
# data_preprocessing
# ---------------------------------------------------------------------------- #
data_transform="${data_transform:-NormalTransform}"

data_preprocessing_args=" data_transform  $data_transform  "



# ---------------------------------------------------------------------------- #
# checkpoint_save
# ---------------------------------------------------------------------------- #
checkpoint_save="${checkpoint_save:-False}"
checkpoint_save_model="${checkpoint_save_model:-False}"
checkpoint_save_optim="${checkpoint_save_optim:-False}"
checkpoint_save_train_metric="${checkpoint_save_train_metric:-False}"
checkpoint_save_test_metric="${checkpoint_save_test_metric:-False}"
checkpoint_root_path="${checkpoint_root_path:-./checkpoints/}"
checkpoint_epoch_list="${checkpoint_epoch_list:-[10,20,30]}"
checkpoint_file_name_save_list="${checkpoint_file_name_save_list:-None}"

checkpoint_args=" checkpoint_save $checkpoint_save  checkpoint_save_model $checkpoint_save_model \
checkpoint_save_optim $checkpoint_save_optim  checkpoint_save_train_metric $checkpoint_save_train_metric \
checkpoint_save_test_metric $checkpoint_save_test_metric  checkpoint_root_path $checkpoint_root_path \
checkpoint_epoch_list $checkpoint_epoch_list  checkpoint_file_name_save_list $checkpoint_file_name_save_list"

# ---------------------------------------------------------------------------- #
# correlation compare layer list
# ---------------------------------------------------------------------------- #
corr_layers_list="${corr_layers_list:-None}"
corr_dataset_len="${corr_dataset_len:-100}"

corr_args=" corr_layers_list $corr_layers_list  corr_dataset_len $corr_dataset_len"




# ---------------------------------------------------------------------------- #
# model
# ---------------------------------------------------------------------------- #
model="${model:-resnet20}"
pretrained="${pretrained:-False}"
pretrained_dir="${pretrained_dir:-no}"

# refer to https://github.com/kevinhsieh/non_iid_dml/blob/master/apps/caffe/examples/cifar10/5parts/resnetgn20_train_val.prototxt.template
group_norm_num="${group_norm_num:-2}"



model_args=" model  $model   pretrained $pretrained   pretrained_dir  $pretrained_dir   \
group_norm_num  $group_norm_num "

# ---------------------------------------------------------------------------- #
# loss function
# ---------------------------------------------------------------------------- #
loss_fn="${loss_fn:-CrossEntropy}"

# ['CrossEntropy', 'nll_loss', 'LDAMLoss', 'local_LDAMLoss',
#        'FocalLoss', 'local_FocalLoss']
imbalance_loss_reweight="${imbalance_loss_reweight:-False}"


loss_fn_args=" loss_fn  $loss_fn  imbalance_loss_reweight $imbalance_loss_reweight "
# ---------------------------------------------------------------------------- #
# trainer
#---------------------------------------------------------------------------- #
# ['normal',  'lstm', 'nas']
trainer_type="${trainer_type:-normal}"

trainer_args=" trainer_type  $trainer_type  "
# ---------------------------------------------------------------------------- #
# algorithm settings
# ---------------------------------------------------------------------------- #
algorithm="${algorithm:-PSGD}"
psgd_exchange="${psgd_exchange:-grad}"  # 'grad', 'model'
psgd_grad_sum="${psgd_grad_sum:-False}"

psgd_grad_debug="${psgd_grad_debug:-False}"
if_get_diff="${if_get_diff:-False}"
exchange_model="${exchange_model:-True}"


# Asynchronous PSGD
# _C.apsgd_exchange = 'grad' # 'grad', 'model' # discarded, use psgd_exchange

# Local SGD
local_round_num="${local_round_num:-4}"

# CHOCO SGD
consensus_stepsize="${consensus_stepsize:-0.5}"
# SAPS FL
bandwidth_type="${bandwidth_type:-random}"   # 'random' 'real'
B_thres="${B_thres:-3.0}"
T_thres="${T_thres:-3}"

# torch_ddp
local_rank="${local_rank:-0}"
init_method="${init_method:-tcp://127.0.0.1:23456}"


# hvd settings and maybe used in future
FP16="${FP16:-False}"
logging_gradients="${logging_gradients:-False}"
merge_threshold="${merge_threshold:-0}"
# horovod version

hvd_origin="${hvd_origin:-False}"
nsteps_update="${nsteps_update:-1}"
# Set it to 1 to turn on momentum_correction
hvd_momentum_correction="${hvd_momentum_correction:-0}"
hvd_is_sparse="${hvd_is_sparse:-False}"





# fedprox
fedprox_mu="${fedprox_mu:-0.1}"


algorithm_args=" algorithm $algorithm  psgd_exchange $psgd_exchange  psgd_grad_sum  $psgd_grad_sum \
psgd_grad_debug  $psgd_grad_debug   if_get_diff  $if_get_diff   exchange_model  $exchange_model \
local_round_num  $local_round_num   consensus_stepsize  $consensus_stepsize  \
bandwidth_type  $bandwidth_type  B_thres  $B_thres  T_thres  $T_thres  \
local_rank  $local_rank  init_method  $init_method  FP16  $FP16 logging_gradients  $logging_gradients \
merge_threshold  $merge_threshold  hvd_origin  $hvd_origin  nsteps_update  $nsteps_update \
hvd_momentum_correction  $hvd_momentum_correction  hvd_is_sparse  $hvd_is_sparse \
fedprox_mu  $fedprox_mu "
# ---------------------------------------------------------------------------- #
# compression Including:
# 'topk','randomk', 'gtopk', 'randomkec',  'eftopk', 'gtopkef'
# 'quantize', 'qsgd', 'sign'
# ---------------------------------------------------------------------------- #
compression="${compression:-no}"
compress_ratio="${compress_ratio:-1.0}"
quantize_level="${quantize_level:-32}"
is_biased="${is_biased:-0}"

compression_args=" compression $compression  compress_ratio $compress_ratio  \
quantize_level $quantize_level  is_biased $is_biased "
# ---------------------------------------------------------------------------- #
# optimizer settings
# comm_round is only used in FedAvg.
# ---------------------------------------------------------------------------- #
epochs="${epochs:-90}"
comm_round="${comm_round:-10}"
# Please indicate which optimizer is used, if no, set it as 'no'
client_optimizer="${client_optimizer:-sgd}"
server_optimizer="${server_optimizer:-no}"

batch_size="${batch_size:-32}"
lr="${lr:-0.1}"
wd="${wd:-0.0001}"
momentum="${momentum:-0.0}"
nesterov="${nesterov:-False}"
clip_grad="${clip_grad:-False}"

optimizer_args=" epochs $epochs   comm_round $comm_round \
client_optimizer $client_optimizer  server_optimizer $server_optimizer \
batch_size $batch_size  lr $lr  wd $wd   momentum  $momentum  nesterov $nesterov \
clip_grad $clip_grad"
# ---------------------------------------------------------------------------- #
# Learning rate schedule parameters
# ---------------------------------------------------------------------------- #
# no (no scheudler), StepLR MultiStepLR  CosineAnnealingLR
sched="${sched:-no}"
lr_decay_rate="${lr_decay_rate:-0.992}"
step_size="${step_size:-1}"

lr_milestones="${lr_milestones:-[30,60]}"
lr_T_max="${lr_T_max:-10}"
lr_eta_min="${lr_eta_min:-0}"
warmup_epochs="${warmup_epochs:-0}"

sched_args=" sched $sched  lr_decay_rate $lr_decay_rate  step_size $step_size \
lr_milestones $lr_milestones  lr_T_max $lr_T_max  lr_eta_min $lr_eta_min  warmup_epochs $warmup_epochs "
# ---------------------------------------------------------------------------- #
# Regularation
# ---------------------------------------------------------------------------- #



# ---------------------------------------------------------------------------- #
# Evaluate settings
# ---------------------------------------------------------------------------- #
frequency_of_the_test="${frequency_of_the_test:-1}"


evaluate_args="  frequency_of_the_test $frequency_of_the_test "
# ---------------------------------------------------------------------------- #
# Robust test
# ---------------------------------------------------------------------------- #
Failure_chance="${Failure_chance:-None}"


# ---------------------------------------------------------------------------- #
# logging
# ---------------------------------------------------------------------------- #
# 'INFO' or 'DEBUG'
level="${level:-INFO}"



# ---------------------------------------------------------------------------- #
# other settings
# ---------------------------------------------------------------------------- #
ci="${ci:-0}"
seed="${seed:-0}"




# =======================================================================================
# get final args
# =======================================================================================




main_args=" $wandb_args \
mode $mode \
$distributed_args \
is_mobile $is_mobile \
$cluster_args \
task $task \
$dataset_args \
$data_sampler_args \
$data_preprocessing_args \
$checkpoint_args \
$corr_args \
$model_args \
$loss_fn_args \
$trainer_args \
$algorithm_args \
$compression_args \
$optimizer_args \
$sched_args \
$evaluate_args \
Failure_chance $Failure_chance \
level $level \
ci $ci \
seed $seed \
"





















